{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f50b872",
   "metadata": {},
   "source": [
    "\n",
    "### Programación de Spark Streaming\n",
    "\n",
    "Spark Streaming es una extensión de la API principal de Spark que permite el procesamiento de secuencias escalables, de alto rendimiento y tolerantes a fallos de transmisiones de datos en vivo. Los datos se pueden ingerir de muchas fuentes como Kafka, Kinesis o sockets TCP, y se pueden procesar utilizando algoritmos complejos expresados con funciones de alto nivel como , , y . Finalmente, los datos procesados se pueden enviar a sistemas de archivos, bases de datos y paneles en vivo. De hecho, puede aplicar los algoritmos de aprendizaje automático y procesamiento de gráficos de Spark en los flujos de datos.mapreducejoinwindow\n",
    "\n",
    "![imgen](https://spark.apache.org/docs/latest/img/streaming-arch.png)\n",
    "\n",
    "Internamente, funciona de la siguiente manera. Spark Streaming recibe flujos de datos de entrada en vivo y divide los datos en lotes, que luego son procesados por el motor Spark para generar el flujo final de resultados en lotes.\n",
    "\n",
    "![imagen](https://spark.apache.org/docs/latest/img/streaming-flow.png)\n",
    "\n",
    "Spark Streaming proporciona una abstracción de alto nivel llamada flujo discretizado o DStream, que representa un flujo continuo de datos. Los DStreams se pueden crear a partir de flujos de datos de entrada de fuentes como Kafka y Kinesis, o mediante la aplicación de operaciones de alto nivel en otros DStreams. Internamente, un DStream se representa como una secuencia de RDD.\n",
    "\n",
    "Esta guía le muestra cómo comenzar a escribir programas de Spark Streaming con DStreams. Puede escribir programas de Spark Streaming en Scala, Java o Python (introducidos en Spark 1.2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a33f3",
   "metadata": {},
   "source": [
    "## Un ejemplo rápido\n",
    "Antes de entrar en los detalles de cómo escribir su propio programa Spark Streaming, echemos un vistazo rápido a cómo se ve un simple programa Spark Streaming. Digamos que queremos contar el número de palabras en los datos de texto recibidos de un servidor de datos que escucha en un socket TCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48407687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33113ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46796e0f",
   "metadata": {},
   "source": [
    "Primero, importamos StreamingContext, que es el punto de entrada principal para toda la funcionalidad de transmisión. Creamos un StreamingContext local con dos subprocesos de ejecución e intervalo por lotes de 1 segundo.\n",
    "\n",
    "El parámetro es un nombre para que la aplicación se muestre en la interfaz de usuario del clúster. es una URL de clúster spark, mesos o YARN, o una cadena especial \"local[*]\" para ejecutarse en modo local. En la práctica, cuando se ejecuta en un clúster, no querrá codificar en el programa, sino más bien iniciar la aplicación con spark-submit y recibirla allí. Sin embargo, para las pruebas locales y las pruebas unitarias, puede pasar \"local[*]\" para ejecutar Spark Streaming en proceso (detecta el número de núcleos en el sistema local).appNamemastermaster\n",
    "\n",
    "El intervalo por lotes debe establecerse en función de los requisitos de latencia de la aplicación y los recursos de clúster disponibles. Consulte la sección Ajuste del rendimiento para obtener más detalles.\n",
    "\n",
    "![image](https://miro.medium.com/max/786/1*p3U__F7ztdmvQR99Hg1-Vw.png)\n",
    "\n",
    "Entonces, los datos comenzarían a verterse en un flujo en lotes, este flujo continuo de datos se llama DStream. Cada lote de dsteam contendría una colección de elementos que se pueden procesar en paralelo, esta colección se llama RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9026121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with two working thread and batch interval of 10 seconds\n",
    "sc = SparkContext(\"local[2]\", \"Contarpalabras\")\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053943d",
   "metadata": {},
   "source": [
    "### Comience a transmitir los datos\n",
    "Para recibir datos, el contexto de transmisión proporciona un método para transmitir datos desde una conexión de socket TCP o desde archivos como orígenes de entrada. Las fuentes pueden ser fuentes como HDFS, S3, etc. Para leer archivos de texto, existe el método textFileStream de javastreamingcontext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48bc96",
   "metadata": {},
   "source": [
    "Pero no podrá leer los archivos ya presentes en el directorio antes de que comience el contexto de transmisión, ya que solo lee los archivos recién creados.\n",
    "\n",
    "Así que aquí transmitiré los datos a través de la conexión de socket a través del puerto 9999 y crearé un receptor java de entrada DStream con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade50be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream('localhost', 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae65d46",
   "metadata": {},
   "source": [
    "Así que ahora, si establece una conexión de socket y escribe algo en el terminal, y ejecuta el dstream, verá que el texto aparece en la consola.\n",
    "\n",
    "Nota: Para iniciar un contexto de transmisión java, debemos decirle a spark que lo inicie, esperar a que finalice el cálculo y luego detenerlo. Y necesitamos imprimir el DStream por el método pprint().\n",
    "\n",
    "Ejecute en el cmd python server_console.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO No ejecutar cuando pase a la siguiente celda\n",
    "lines.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df55fd",
   "metadata": {},
   "source": [
    "Observe cuando imprime la salida en el tiempo t1, pero no se imprime ninguna salida en el tiempo t2 y t3, ya que obtiene datos para cada 10 segundos. En los siguientes intervalos de lote, no recibió ninguna entrada, por lo que no imprime nada.\n",
    "\n",
    "Ahora te mostraré cómo podemos usar algunas transformaciones en estos dstreams usando funciones lanbda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca86f4",
   "metadata": {},
   "source": [
    "### Transformación  map\n",
    "\n",
    "La transformación de mapas aplica la función que especificamos en el DStream y produce un valor de salida para cada valor de entrada. Así que básicamente transforma un flujo en otro. Al igual que aquí, quiero contar la longitud de la línea de texto, así que usaré la transformación de mapas para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO No ejecutar cuando pase a la siguiente celda\n",
    "length = lines.map(lambda x: len(x))\n",
    "length.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad1818",
   "metadata": {},
   "source": [
    "### Transformación de FlatMap\n",
    "\n",
    "La transformación FlatMap aplica la función en DStream, pero puede producir uno o más valores de salida para cada valor de entrada. Entonces, si quiero transformar el RDD de tal manera que produzca más de un valor, usaré la transformación FlatMap.\n",
    "\n",
    "Así que aquí le di a la entrada una línea de texto 'hola cómo estás' y quiero dividirla en palabras individuales. Utilicé la función lambda para lo mismo. Una transformación FlatMap devuelve un número arbitrario de valores que depende del rdd y la función aplicada, por lo que el tipo devuelto tiene que ser un flujo de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1790f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = lines.flatMap(lambda x: x.split(\" \"))\n",
    "palabras.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee941de7",
   "metadata": {},
   "source": [
    "### Transformación de filtros\n",
    "\n",
    "La transformación del filtro filtra el DStream de acuerdo con la función dada. Al igual que después de la transformación de flatMap, digamos que quiero filtrar las lineas que empiezan en A del flujo de palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = lines.filter(lambda x: x.startswith('A'))\n",
    "output.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c22159",
   "metadata": {},
   "source": [
    "Una vez definido un contexto, debe hacer lo siguiente.\n",
    "\n",
    "Defina las fuentes de entrada creando DStreams de entrada.\n",
    "Defina los cálculos de streaming aplicando operaciones de transformación y salida a DStreams.\n",
    "Comience a recibir datos y procesarlos utilizando .streamingContext.start()\n",
    "Espere a que se detenga el procesamiento (manualmente o debido a cualquier error) utilizando .streamingContext.awaitTermination()\n",
    "El procesamiento se puede detener manualmente utilizando .streamingContext.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa059737",
   "metadata": {},
   "source": [
    "Usando este contexto, podemos crear un DStream que represente la transmisión de datos desde una fuente TCP, especificada como nombre de host (\"127.0.0.1\" ) y puerto (65432 ). 127.0.0.1:65432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 65432)\n",
    "#lines = ssc.socketTextStream(\"rtd.hpwren.ucsd.edu\", 12020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb6e73",
   "metadata": {},
   "source": [
    "Este DStream representa el flujo de datos que se recibirá del servidor de datos. Cada registro de este DStream es una línea de texto. A continuación, queremos dividir las líneas por espacio en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1001084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1bd79",
   "metadata": {},
   "source": [
    "flatMap es una operación DStream de uno a varios que crea un nuevo DStream generando varios registros nuevos a partir de cada registro en el DStream de origen. En este caso, cada línea se dividirá en varias palabras y el flujo de palabras se representará como DStream. A continuación, queremos contar estas palabras.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e961b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d4cb6",
   "metadata": {},
   "source": [
    "El DStream se asigna aún más (transformación uno a uno) a un DStream de pares, que luego se reduce para obtener la frecuencia de las palabras en cada lote de datos. Por último, imprimirá algunos de los recuentos generados cada segundo.words(word, 1)wordCounts.print()\n",
    "\n",
    "Tenga en cuenta que cuando se ejecutan estas líneas, Spark Streaming solo configura el cálculo que realizará cuando se inicie y aún no se ha iniciado ningún procesamiento real. Para iniciar el procesamiento después de que se hayan configurado todas las transformaciones, finalmente llamamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3645b47",
   "metadata": {},
   "source": [
    "Cree una sesion cmd y ejecute python server_send.py, para leer el libro de Don Quijote y enviar las lineas a Stream de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17fac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Puntos a recordar:\n",
    "Una vez que se ha iniciado un contexto, no se pueden configurar ni agregar nuevos cálculos de transmisión.\n",
    "Una vez que se ha detenido un contexto, no se puede reiniciar.\n",
    "Solo un StreamingContext puede estar activo en una JVM al mismo tiempo.\n",
    "\n",
    "stop() en StreamingContext también detiene SparkContext. Para detener solo StreamingContext, establezca el parámetro opcional de called en false.stop()stopSparkContext\n",
    "Un SparkContext se puede reutilizar para crear varios StreamingContexts, siempre y cuando el StreamingContext anterior se detenga (sin detener el SparkContext) antes de que se cree el siguiente StreamingContext.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
